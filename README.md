
# Micrograd From Scratch (Neural Network + Autograd Engine)

This project is a **from-scratch reimplementation of a tiny deep learning framework** inspired by PyTorch and Karpathyâ€™s micrograd.

It includes:
- A scalar-based **automatic differentiation engine**
- A **neural network library** (Neuron, Layer, MLP)
- An **SGD optimizer**
- An **MSE loss**
- A **full training loop** that learns XOR

---

# ğŸ“’ Table of Contents

- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Training Example (XOR)](#training-example-xor)
- [Contributing](#contributing)

---

# ğŸ“¦ Installation

1. Clone the repo:

   ```bash
   git clone https://github.com/sakshamp00/micrograd.git
   cd micrograd
   ```

2. Install dependencies (optional, mainly for plotting):

    ```bash
    pip install matplotlib
    ```

---

# ğŸ› ï¸ Usage

Just run the training script:

    python train_xor.py
    

Youâ€™ll see:
- Loss decreasing over training steps
- Final predictions on the XOR dataset
- Optional loss plot (if matplotlib is installed)

---


# ğŸ“ Project Structure

```micrograd/
â”œâ”€â”€ engine.py       # Core autograd Value class
â”œâ”€â”€ nn.py           # Neuron, Layer, MLP
â”œâ”€â”€ optim.py        # SGD optimizer
â”œâ”€â”€ loss.py         # Loss functions (MSE)
â”œâ”€â”€ train_xor.py    # Training script
â””â”€â”€ README.md       # This file
```

---

# ğŸ“Š Training Example (XOR)

XOR dataset:

```
0 âŠ• 0 â†’ 0  
0 âŠ• 1 â†’ 1  
1 âŠ• 0 â†’ 1  
1 âŠ• 1 â†’ 0
```
The training script:
- Builds the MLP
- Loops forward â†’ backprop â†’ update
- Prints loss and final accuracy

Example output after training:

```
step 0, loss = 2.31  
step 100, loss = 0.21 
...
step 900, loss = 0.02  

Trained model predictions:
Input: [0.0, 0.0], Predicted: 0.0111, True: 0.0
Input: [0.0, 1.0], Predicted: 0.9785, True: 1.0
Input: [1.0, 0.0], Predicted: 0.9831, True: 1.0
Input: [1.0, 1.0], Predicted: 0.0142, True: 0.0
```
---
# ğŸ¤ Contributing

Contributions are welcome! Whether itâ€™s improving the documentation, adding features like:

- Activation functions (ReLU, Sigmoid)
- Optimizers (Adam)
- Batch support
- More demos (MNIST, regression)

Feel free to open issues or pull requests ğŸ‰